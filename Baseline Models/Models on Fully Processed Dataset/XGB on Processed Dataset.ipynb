{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O4Zdpj42Q-4L","outputId":"b7d23cba-87bc-44db-962d-3806c20d267c","executionInfo":{"status":"ok","timestamp":1730106873825,"user_tz":-480,"elapsed":47175,"user":{"displayName":"Adithya R","userId":"06981244434554202327"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","tqdm is already installed.\n","\n","pyspark is NOT installed. Installing now...\n","pyspark installation completed.\n","\n","gdown is already installed.\n","\n","numpy is already installed.\n","\n","xgboost is NOT installed. Installing now...\n","xgboost installation completed.\n","\n","sparkxgb is NOT installed. Installing now...\n","sparkxgb installation completed.\n"]}],"source":["import importlib\n","import subprocess\n","import sys\n","import gc\n","\n","def check_and_install_package(package_name, version=None):\n","    try:\n","        importlib.import_module(package_name)\n","        print(f\"\\n{package_name} is already installed.\")\n","    except ImportError:\n","        print(f\"\\n{package_name} is NOT installed. Installing now...\")\n","        if version:\n","            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", f\"{package_name}=={version}\"])\n","        else:\n","            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n","        print(f\"{package_name} installation completed.\")\n","\n","# List of packages to check along with specific versions if necessary\n","packages = [\n","    {\"name\": \"tqdm\", \"version\": None},\n","    {\"name\": \"pyspark\", \"version\": \"3.1.1\"},\n","    {\"name\": \"gdown\", \"version\": None},\n","    {\"name\": \"numpy\", \"version\": \"1.23.5\"},\n","    {\"name\": \"xgboost\", \"version\": None},\n","    {\"name\": \"sparkxgb\", \"version\": None},\n","]\n","\n","# Checking and installing the packages\n","for package in packages:\n","    check_and_install_package(package[\"name\"], package[\"version\"])"]},{"cell_type":"code","source":["!pip install sparkxgb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CPD4U30UJDrh","executionInfo":{"status":"ok","timestamp":1730106875330,"user_tz":-480,"elapsed":1507,"user":{"displayName":"Adithya R","userId":"06981244434554202327"}},"outputId":"4fd9a47e-ec08-42d5-d946-9a1d25d6627a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sparkxgb in /usr/local/lib/python3.10/dist-packages (0.1)\n","Requirement already satisfied: pyspark==3.1.1 in /usr/local/lib/python3.10/dist-packages (from sparkxgb) (3.1.1)\n","Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.1.1->sparkxgb) (0.10.9)\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"RzKYWgsyRGny","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730106892791,"user_tz":-480,"elapsed":17464,"user":{"displayName":"Adithya R","userId":"06981244434554202327"}},"outputId":"c131f644-b0a2-41e2-9d6e-95975266c19c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1184,"status":"ok","timestamp":1730106893974,"user":{"displayName":"Adithya R","userId":"06981244434554202327"},"user_tz":-480},"id":"4QHkLfzsGy18","outputId":"5c15a299-79f8-426d-8bfb-4348542b6b1a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Jar Files copied to: /resources\n","['xgboost4j-spark_2.12-1.7.6.jar', 'xgboost4j_2.12-1.7.6.jar']\n"]}],"source":["import os\n","import shutil\n","\n","# Defining local resources directory\n","local_resources_path = \"/resources\"\n","os.makedirs(local_resources_path, exist_ok=True)\n","\n","# Defining the source paths from your mounted Google Drive\n","xgboost4j_source = \"/content/drive/MyDrive/Big Data Analytics - Project/resources/xgboost4j_2.12-1.7.6.jar\"\n","xgboost4j_spark_source = \"/content/drive/MyDrive/Big Data Analytics - Project/resources/xgboost4j-spark_2.12-1.7.6.jar\"\n","\n","# Defining the destination paths in the instance's local file system\n","xgboost4j_dest = os.path.join(local_resources_path, \"xgboost4j_2.12-1.7.6.jar\")\n","xgboost4j_spark_dest = os.path.join(local_resources_path, \"xgboost4j-spark_2.12-1.7.6.jar\")\n","\n","# Copying the files from Google Drive to the local instance\n","shutil.copyfile(xgboost4j_source, xgboost4j_dest)\n","shutil.copyfile(xgboost4j_spark_source, xgboost4j_spark_dest)\n","\n","# Verifying that the files are copied\n","print(f\"Jar Files copied to: {local_resources_path}\")\n","print(os.listdir(local_resources_path))\n"]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"FlmHOXXgOmU3"}},{"cell_type":"markdown","source":["**Testing if spark XGB works on dummy data**"],"metadata":{"id":"Epdaw4bNh_xS"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.ml.feature import VectorAssembler, StandardScaler\n","from pyspark.sql import Row\n","from pyspark.ml.evaluation import RegressionEvaluator\n","from sparkxgb import XGBoostRegressor\n","\n","# Path to the copied JAR files\n","xgboost4j_jar = \"/resources/xgboost4j_2.12-1.7.6.jar\"\n","xgboost4j_spark_jar = \"/resources/xgboost4j-spark_2.12-1.7.6.jar\"\n","\n","# Initializing Spark session\n","spark = SparkSession.builder \\\n","    .appName(\"Test_SparkXGB\") \\\n","    .config(\"spark.jars\", f\"{xgboost4j_jar},{xgboost4j_spark_jar}\") \\\n","    .getOrCreate()\n","\n","# Creating some dummy data for testing\n","data = [\n","    Row(price=30000, feature1=4.0, feature2=1.2),\n","    Row(price=25000, feature1=5.0, feature2=1.5),\n","    Row(price=22000, feature1=6.0, feature2=1.7),\n","    Row(price=35000, feature1=3.0, feature2=1.1),\n","    Row(price=28000, feature1=7.0, feature2=1.9),\n","    Row(price=32000, feature1=8.0, feature2=2.0),\n","    Row(price=27000, feature1=5.5, feature2=1.8),\n","]\n","\n","# Creating DataFrame\n","df = spark.createDataFrame(data)\n","\n","# Assembling features\n","assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\"], outputCol=\"features\")\n","df_assembled = assembler.transform(df)\n","\n","# Scaling features\n","scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n","scaler_model = scaler.fit(df_assembled)\n","df_scaled = scaler_model.transform(df_assembled)\n","\n","# Splitting data\n","train_df, test_df = df_scaled.randomSplit([0.8, 0.2], seed=42)\n","\n","# Defining XGBoost Regressor (from sparkxgb)\n","xgb_regressor = XGBoostRegressor(\n","    featuresCol=\"scaled_features\",\n","    labelCol=\"price\",\n","    maxDepth=6,\n","    numRound=100,\n","    objective=\"reg:squarederror\",\n",")\n","\n","# Training the model\n","model = xgb_regressor.fit(train_df)\n","\n","# Making predictions\n","predictions = model.transform(test_df)\n","\n","# Evaluating the model\n","evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n","r2 = evaluator.evaluate(predictions)\n","print(f\"R-Squared Score: {r2}\")\n","\n","# Showing predictions\n","predictions.select(\"price\", \"prediction\").show()\n","\n","spark.catalog.clearCache()\n","spark.stop()\n","print(\"Test_SparkXGB Stopped !\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hJUJ-2E3H2Kw","executionInfo":{"status":"ok","timestamp":1727233791538,"user_tz":-480,"elapsed":2318,"user":{"displayName":"Adithya R","userId":"06981244434554202327"}},"outputId":"e226fc9f-0283-416c-fefb-26541e55aa98"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["R-Squared Score: -9.000014062505493\n","+-----+---------------+\n","|price|     prediction|\n","+-----+---------------+\n","|30000|24999.994140625|\n","|35000|24999.994140625|\n","+-----+---------------+\n","\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"J346RmUTOnXF"}},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7862,"status":"ok","timestamp":1730106901835,"user":{"displayName":"Adithya R","userId":"06981244434554202327"},"user_tz":-480},"id":"dMvRtGAZRWKh","outputId":"6202f900-74b1-4e12-dc31-b4c5eaa2dbd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Spark session started with version: 3.1.1\n"]}],"source":["from pyspark.sql import SparkSession\n","\n","# Defining the path to the copied jar files in the local instance\n","jar_files = \"/resources/xgboost4j_2.12-1.7.6.jar,/resources/xgboost4j-spark_2.12-1.7.6.jar\"\n","\n","# Initializing Spark session with the JAR files\n","spark = SparkSession.builder \\\n","    .appName(\"XGBoostRegressor\") \\\n","    .config(\"spark.driver.memory\", \"120g\") \\\n","    .config(\"spark.executor.memory\", \"120g\") \\\n","    .config(\"spark.driver.maxResultSize\", \"40g\") \\\n","    .config(\"spark.executor.memoryOverhead\", \"40g\") \\\n","    .config(\"spark.executor.cores\", \"5\") \\\n","    .config(\"spark.kryoserializer.buffer.max\", \"2047m\") \\\n","    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n","    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n","    .config(\"spark.sql.shuffle.partitions\", \"400\") \\\n","    .config(\"spark.hadoop.fs.file.impl\", \"org.apache.hadoop.fs.LocalFileSystem\") \\\n","    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThreads=4 -XX:ParallelGCThreads=4\") \\\n","    .config(\"spark.jars\", jar_files) \\\n","    .getOrCreate()\n","\n","# Verifying Spark session creation\n","print(f\"Spark session started with version: {spark.version}\")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1730106901835,"user":{"displayName":"Adithya R","userId":"06981244434554202327"},"user_tz":-480},"id":"x5Z292PcHRYR","outputId":"38632938-91be-4e5c-bbd0-d500fe473587"},"outputs":[{"output_type":"stream","name":"stdout","text":["sparkxgb loaded successfully!\n"]}],"source":["# Testing if sparkxgb is loaded properly\n","try:\n","    from sparkxgb import XGBoostRegressor\n","\n","    # Create a test XGBoost model using sparkxgb\n","    model = XGBoostRegressor()\n","    print(\"sparkxgb loaded successfully!\")\n","except Exception as e:\n","    print(f\"Error loading sparkxgb: {e}\")\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"dCaWvkRDRw1H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730106984305,"user_tz":-480,"elapsed":82472,"user":{"displayName":"Adithya R","userId":"06981244434554202327"}},"outputId":"5eef9cb7-fb5f-4def-e6d0-b6a54b86f84b"},"outputs":[{"output_type":"stream","name":"stdout","text":["The Processed DataFrame has been loaded successfully.\n"]}],"source":["# loading the df\n","\n","!cp '/content/drive/MyDrive/Big Data Analytics - Project/Datasets/Processed_DF.parquet' /content/\n","\n","output_path = '/content/Processed_DF.parquet'\n","df = spark.read.parquet(output_path)\n","print(\"The Processed DataFrame has been loaded successfully.\")\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"LZ1dUb5kRzBz","executionInfo":{"status":"ok","timestamp":1730106984306,"user_tz":-480,"elapsed":3,"user":{"displayName":"Adithya R","userId":"06981244434554202327"}}},"outputs":[],"source":["df = df.repartition(100)  # Repartitioning into 100 partitions for parallelism"]},{"cell_type":"code","source":["# Printing the shape of the DataFrame\n","total_rows = df.count()\n","total_columns = len(df.columns)\n","\n","print(f\"The shape of the loaded DataFrame is: ({total_rows}, {total_columns})\")"],"metadata":{"id":"IwSMZkPSQKW9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730106988881,"user_tz":-480,"elapsed":4578,"user":{"displayName":"Adithya R","userId":"06981244434554202327"}},"outputId":"4c529e15-9c35-4f58-ed5b-0642ef48d7c6"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["The shape of the loaded DataFrame is: (3000040, 42)\n"]}]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4897,"status":"ok","timestamp":1730106993777,"user":{"displayName":"Adithya R","userId":"06981244434554202327"},"user_tz":-480},"id":"s4W_FzgxSKhL","outputId":"c8ff18ac-da6e-4557-fa42-80f54997f363"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of rows with at least one null value: 0\n"]}],"source":["from functools import reduce\n","from pyspark.sql.functions import col\n","\n","# Checking for rows with at least one null value in any column\n","rows_with_null = df.filter(\n","    reduce(lambda x, y: x | y, (col(c).isNull() for c in df.columns))).count()\n","\n","print(f\"Number of rows with at least one null value: {rows_with_null}\")"]},{"cell_type":"markdown","source":["### **Handling Categorical Coloumns**"],"metadata":{"id":"hEAdCyX20Z9f"}},{"cell_type":"code","execution_count":11,"metadata":{"id":"xmEHi2yiR2Mp","executionInfo":{"status":"ok","timestamp":1730106993777,"user_tz":-480,"elapsed":5,"user":{"displayName":"Adithya R","userId":"06981244434554202327"}}},"outputs":[],"source":["df=df.drop('description','major_options','mileage')\n","# Keeping the columns ['exterior_color','dealer_zip','interior_color']"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4323,"status":"ok","timestamp":1730106998097,"user":{"displayName":"Adithya R","userId":"06981244434554202327"},"user_tz":-480},"id":"ib1FhenhSMIy","outputId":"15fbe6c8-4168-4f10-9f9e-5c8a8263be03"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unique exterior colors: 23036\n","Unique interior colors: 38528\n"]}],"source":["# Counting unique values in 'exterior_color' and 'interior_color' columns\n","exterior_colors_count = df.select('exterior_color').distinct().count()\n","interior_colors_count = df.select('interior_color').distinct().count()\n","\n","print(f\"Unique exterior colors: {exterior_colors_count}\")\n","print(f\"Unique interior colors: {interior_colors_count}\")"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"T9PceT2CSN3i","executionInfo":{"status":"ok","timestamp":1730106998097,"user_tz":-480,"elapsed":2,"user":{"displayName":"Adithya R","userId":"06981244434554202327"}}},"outputs":[],"source":["from pyspark.sql import functions as F\n","from pyspark.sql.types import ArrayType, StringType\n","\n","# Listing top colors for both exterior and interior\n","colors = ['White', 'Black', 'Gray', 'Silver', 'Red', 'Blue', 'Brown', 'Green', 'Beige', 'Orange', 'Gold', 'Yellow', 'Purple']\n","\n","# Creating a UDF to find colors in the color columns\n","@F.udf(returnType=ArrayType(StringType()))\n","def find_colors(color_string):\n","    if color_string is None or color_string.strip() == \"\":\n","        return [\"Other\"]  # Handle empty or null values\n","    found_colors = [c for c in colors if c.lower() in color_string.lower()]\n","    return found_colors if found_colors else [\"Other\"]  # Label non-matching colors as \"Other\"\n","\n","# Applying the UDF to both the exterior and interior color columns\n","df = df.withColumn(\"exterior_color_array\", find_colors(\"exterior_color\"))\n","df = df.withColumn(\"interior_color_array\", find_colors(\"interior_color\"))\n","\n","# Creating a column with the count of colors found for both exterior and interior\n","df = df.withColumn(\"exterior_color_count\", F.size(\"exterior_color_array\"))\n","df = df.withColumn(\"interior_color_count\", F.size(\"interior_color_array\"))\n","\n","# Joining the color arrays into string columns\n","df = df.withColumn(\"exterior_color\", F.array_join(\"exterior_color_array\", \", \"))\n","df = df.withColumn(\"interior_color\", F.array_join(\"interior_color_array\", \", \"))\n","\n","# Labeling mixed colors for both exterior and interior colors\n","df = df.withColumn(\n","    \"exterior_color\",\n","    F.when(F.col(\"exterior_color_count\") > 1, \"Mixed Colors\")\n","     .otherwise(F.col(\"exterior_color\")))\n","\n","df = df.withColumn(\n","    \"interior_color\",\n","    F.when(F.col(\"interior_color_count\") > 1, \"Mixed Colors\")\n","     .otherwise(F.col(\"interior_color\")))\n","\n","# Dropping temporary columns\n","df = df.drop(\"exterior_color_array\", \"exterior_color_count\", \"interior_color_array\", \"interior_color_count\")\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5291,"status":"ok","timestamp":1730107003386,"user":{"displayName":"Adithya R","userId":"06981244434554202327"},"user_tz":-480},"id":"b4EcpFeISPsU","outputId":"b6ee8f8a-e59f-4119-f175-badd587c8c3a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Exterior Color Distribution:\n","+--------------+------+----------+\n","|exterior_color|count |percentage|\n","+--------------+------+----------+\n","|White         |675979|22.53     |\n","|Black         |580148|19.34     |\n","|Other         |543638|18.12     |\n","|Silver        |384540|12.82     |\n","|Blue          |253263|8.44      |\n","|Red           |242331|8.08      |\n","|Gray          |231172|7.71      |\n","|Green         |23026 |0.77      |\n","|Mixed Colors  |19728 |0.66      |\n","|Brown         |12905 |0.43      |\n","|Orange        |11638 |0.39      |\n","|Gold          |10544 |0.35      |\n","|Beige         |5065  |0.17      |\n","|Yellow        |4855  |0.16      |\n","|Purple        |1208  |0.04      |\n","+--------------+------+----------+\n","\n"]}],"source":["# Counting the occurrences of each exterior and interior color and calculating percentages\n","exterior_color_counts = df.groupBy(\"exterior_color\").count().withColumn(\n","    \"percentage\", F.round((F.col(\"count\") / df.count()) * 100, 2))\n","\n","interior_color_counts = df.groupBy(\"interior_color\").count().withColumn(\n","    \"percentage\", F.round((F.col(\"count\") / df.count()) * 100, 2))\n","\n","\n","# Showing the results\n","print(\"Exterior Color Distribution:\")\n","exterior_color_counts.orderBy(F.desc(\"count\")).show(truncate=False)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1568,"status":"ok","timestamp":1730107004952,"user":{"displayName":"Adithya R","userId":"06981244434554202327"},"user_tz":-480},"id":"4XTgdusCSRQe","outputId":"645c4e1c-6d0d-4ee4-d544-7a7570807b86"},"outputs":[{"output_type":"stream","name":"stdout","text":["Interior Color Distribution:\n","+--------------+-------+----------+\n","|interior_color|count  |percentage|\n","+--------------+-------+----------+\n","|Black         |1624033|54.13     |\n","|Other         |577578 |19.25     |\n","|Gray          |383966 |12.8      |\n","|Mixed Colors  |171212 |5.71      |\n","|White         |91545  |3.05      |\n","|Brown         |65943  |2.2       |\n","|Red           |34117  |1.14      |\n","|Silver        |24124  |0.8       |\n","|Blue          |22828  |0.76      |\n","|Green         |2048   |0.07      |\n","|Gold          |1193   |0.04      |\n","|Orange        |1133   |0.04      |\n","|Yellow        |134    |0.0       |\n","|Purple        |121    |0.0       |\n","|Beige         |65     |0.0       |\n","+--------------+-------+----------+\n","\n"]}],"source":["print(\"Interior Color Distribution:\")\n","interior_color_counts.orderBy(F.desc(\"count\")).show(truncate=False)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":343,"status":"ok","timestamp":1730107005294,"user":{"displayName":"Adithya R","userId":"06981244434554202327"},"user_tz":-480},"id":"jLYLAYhwSURW","outputId":"e3ebbfa7-ee66-41f9-de6a-59b065248806"},"outputs":[{"output_type":"stream","name":"stdout","text":["Final processed DataFrame used for the model has 3000040 rows and 39 columns.\n"]}],"source":["print(f\"Final processed DataFrame used for the model has {df.count()} rows and {len(df.columns)} columns.\")"]},{"cell_type":"code","source":["# Calculating the average price\n","avg_price = df.agg({\"price\": \"avg\"}).collect()[0][0]\n","print(f\"Average price of a car: {round(avg_price)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7dcz6FESaJZt","executionInfo":{"status":"ok","timestamp":1730107006287,"user_tz":-480,"elapsed":994,"user":{"displayName":"Adithya R","userId":"06981244434554202327"}},"outputId":"f1979ead-9b20-4ff0-89aa-d4e08f2d75ed"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Average price of a car: 29933\n"]}]},{"cell_type":"markdown","metadata":{"id":"P9W2rhuySne7"},"source":["# **XGB**"]},{"cell_type":"markdown","metadata":{"id":"fWb14wEQIKEf"},"source":["## **using XGBoostRegressor**"]},{"cell_type":"markdown","source":["## **10% of the Dataset**"],"metadata":{"id":"Gmst3NTc8kKq"}},{"cell_type":"code","source":["import warnings\n","from tqdm import tqdm\n","from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler, OneHotEncoder\n","from pyspark.ml import Pipeline\n","from pyspark.sql.functions import mean as sql_mean\n","import pyspark.sql.functions as F\n","\n","# Ignore warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"Processing the data...\")\n","with tqdm(total=6, desc=\"Progress\") as pbar:\n","\n","    df_sample = df.sample(fraction=0.1, seed=42)   # Randomly sampling 10% of the data\n","    pbar.update(1)\n","\n","    # Handling categorical columns\n","    cat_columns = [field for (field, dtype) in df_sample.dtypes if dtype == \"string\"]\n","    stages = []\n","    for col_name in cat_columns:\n","        indexer = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_indexed\", handleInvalid=\"keep\")\n","        encoder = OneHotEncoder(inputCol=f\"{col_name}_indexed\", outputCol=f\"{col_name}_encoded\")\n","        stages += [indexer, encoder]\n","    pbar.update(1)\n","\n","    # Assembling features\n","    num_columns = [col for col in df_sample.columns if col != 'price' and col not in cat_columns]\n","    encoded_columns = [f\"{col}_encoded\" for col in cat_columns]\n","    feature_columns = num_columns + encoded_columns\n","    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n","    stages += [assembler]\n","    pbar.update(1)\n","\n","    # Adding scaling to the pipeline\n","    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n","    stages += [scaler]\n","\n","    # Creating and apply the pipeline\n","    pipeline = Pipeline(stages=stages)\n","    pipeline_model = pipeline.fit(df_sample)\n","    df_sample = pipeline_model.transform(df_sample)\n","    pbar.update(1)\n","\n","    # Filling in missing values\n","    for col in df_sample.columns:\n","        if df_sample.schema[col].dataType.typeName() in [\"double\", \"float\", \"int\", \"long\"]:\n","            mean_value = df_sample.select(sql_mean(col)).first()[0]\n","            df_sample = df_sample.na.fill({col: mean_value})\n","    pbar.update(1)\n","\n","    # Splitting the data\n","    train_df, test_df = df_sample.randomSplit([0.8, 0.2], seed=42)\n","    pbar.update(1)\n","\n","print(\"\\n\\nData preprocessing and splitting completed!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AV_kj0yB_jlq","executionInfo":{"status":"ok","timestamp":1730107132667,"user_tz":-480,"elapsed":56619,"user":{"displayName":"Adithya R","userId":"06981244434554202327"}},"outputId":"b870bf5f-6ae3-4c08-d7d0-b9f6d3dba543"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing the data...\n"]},{"output_type":"stream","name":"stderr","text":["Progress: 100%|██████████| 6/6 [00:56<00:00,  9.38s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Data preprocessing and splitting completed!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["print(f\"Train_DF has {train_df.count()} rows and {len(train_df.columns)} columns\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eo-Ldg0d_nhX","executionInfo":{"status":"ok","timestamp":1727234429605,"user_tz":-480,"elapsed":221031,"user":{"displayName":"Adithya R","userId":"06981244434554202327"}},"outputId":"b090e98e-dda6-4fc4-b0a2-b2d6aeaa2a1b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train_DF has 240412 rows and 71 columns\n"]}]},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n","from sparkxgb import XGBoostRegressor\n","import time\n","\n","# Model training\n","print(\"Training XGBoost model...\")\n","\n","xgb_regressor = XGBoostRegressor(\n","    featuresCol=\"scaled_features\",  # Use scaled features\n","    labelCol=\"price\",               # Target column\n","    maxDepth=6,\n","    numRound=100,\n","    objective=\"reg:squarederror\",   # Regression task\n","    treeMethod=\"hist\",\n",")\n","\n","\n","# Before training\n","start_time = time.time()\n","\n","# Training the model\n","model = xgb_regressor.fit(train_df)\n","\n","# Making predictions\n","print(\"Making predictions...\")\n","predictions = model.transform(test_df)\n","\n","# Evaluating the model\n","print(\"Evaluating the model...\")\n","evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n","r2 = evaluator.evaluate(predictions)\n","\n","print(f\"\\nTrain size: {train_df.count()} samples\")\n","print(f\"Test size: {test_df.count()} samples\")\n","print(f\"\\n\\nR-Squared Score (Accuracy): {round(r2 * 100)}%\\n\")\n","\n","# Calculating total runtime\n","end_time = time.time()\n","total_runtime = (end_time - start_time) / 60\n","print(f\"\\nOverall runtime: {round(total_runtime)} minutes.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pWmfQjnZ_qDw","executionInfo":{"status":"ok","timestamp":1727242039252,"user_tz":-480,"elapsed":7454957,"user":{"displayName":"Adithya R","userId":"06981244434554202327"}},"outputId":"6ba8c678-be0d-4383-fa98-0d02ac8d7d29"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training XGBoost model...\n","Making predictions...\n","Evaluating the model...\n","\n","Train size: 240412 samples\n","Test size: 60072 samples\n","\n","\n","R-Squared Score (Accuracy): 85%\n","\n","\n","Overall runtime: 124 minutes.\n"]}]},{"cell_type":"code","source":["# Calculating additional metrics\n","mae_evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"mae\")\n","mae = mae_evaluator.evaluate(predictions)\n","\n","mse_evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"mse\")\n","mse = mse_evaluator.evaluate(predictions)\n","\n","rmse_evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n","rmse = rmse_evaluator.evaluate(predictions)\n","\n","print(\"Additional Metrics:\")\n","print(f\"Mean Absolute Error: {round(mae)}\")\n","print(f\"Mean Squared Error: {round(mse)}\")\n","print(f\"Root Mean Squared Error: {round(rmse)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wlJVKPLB_2bJ","executionInfo":{"status":"ok","timestamp":1727245088184,"user_tz":-480,"elapsed":2978772,"user":{"displayName":"Adithya R","userId":"06981244434554202327"}},"outputId":"36c3b9a7-aab8-4f4d-f40a-7c7700aca12f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Additional Metrics:\n","Mean Absolute Error: 3066\n","Mean Squared Error: 56468542\n","Root Mean Squared Error: 7515\n"]}]},{"cell_type":"markdown","source":["The Mean Absolute Error (`MAE`) of **\\$3066**  suggests that, on average, the predicted car prices deviate from the actual prices by this amount. Given that the Mean car price is **\\$29,933** , this error represents about `10.2% of the mean price`, which suggests that the model performs extremely well."],"metadata":{"id":"dUtNcdT9eNHf"}},{"cell_type":"code","source":["# Getting feature importances from the loaded native XGBoost model\n","importance_dict = native_model.get_score(importance_type='weight')\n","\n","features_list = pipeline_model.stages[-2].getInputCols()  # Get the input column names from the VectorAssembler\n","\n","# Mapping the feature indices (f0, f1, ...) to the actual feature names safely\n","sorted_importance = [\n","    (features_list[int(f[1:])], importance)\n","    for f, importance in importance_dict.items()\n","    if int(f[1:]) < len(features_list)  # Ensuring the index is within bounds\n","]\n","\n","# Sorting by importance\n","sorted_importance = sorted(sorted_importance, key=lambda x: x[1], reverse=True)\n","\n","# Printing the top 10 features with their actual names (sorted by importance)\n","print(\"Top 10 Features Ranked by Importance (Highest to Lowest)\")\n","for rank, (feature, importance) in enumerate(sorted_importance[:10], 1):\n","    print(f\"{rank}. {feature}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AzBHII_yh4wO","executionInfo":{"status":"ok","timestamp":1727245353693,"user_tz":-480,"elapsed":335,"user":{"displayName":"Adithya R","userId":"06981244434554202327"}},"outputId":"98a607c1-796a-48d7-a2c1-37172a5a1c9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Top 10 Features Ranked by Importance (Highest to Lowest)\n","1. log_mileage\n","2. daysonmarket\n","3. city_fuel_economy\n","4. year\n","5. major_options_count\n","6. latitude\n","7. horsepower\n","8. engine_displacement\n","9. height\n","10. longitude\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"ZbclCJ4O8o6c"}}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","machine_shape":"hm","provenance":[{"file_id":"1-U4qBE_z5tjZUFTTCfv2skujXMGBlW8M","timestamp":1727142930071}],"authorship_tag":"ABX9TyOh2TePigL34Qd5mCNpqlr8"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}